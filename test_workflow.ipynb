{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from blobster import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pydyverse\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "import io\n",
    "import re\n",
    "import zipfile\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "from functools import wraps\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas_profiling\n",
    "import pandas_flavor as pf\n",
    "import jdc\n",
    "\n",
    "class Datapipe:\n",
    "    def __init__(self, service=None, entity=None, meta=None, runtime=None):\n",
    "        self.service: str = service\n",
    "        self.runtime: str = runtime\n",
    "        self.entity: str = entity\n",
    "        self.meta = meta\n",
    "        self.today = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "        self.profile_report = None\n",
    "\n",
    "datapipe = Datapipe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#blobster\n",
    "azure_blob_storage = AzureBlobStorage(account='vfudevblobstorage',\n",
    "                                      key='eEbmjhexKPoGy+e8B2aUiDBPIG0qRThKRenJLgVbGN2TlcwMty9NopeUmwzIQ67DpYvcr8AwgPjobAiT0Zmzkw==')\n",
    "azure_blob_storage.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wf\n",
    "datapipe.service = 'vfu'\n",
    "datapipe.runtime = 'local'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'meta' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-81f8755c9679>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmeta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'meta' is not defined"
     ]
    }
   ],
   "source": [
    "meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vfu\n",
    "@pf.register_dataframe_method\n",
    "def step_consolidate_item_learning_levels(df, col, mapping=None):\n",
    "    if mapping:\n",
    "        learning_level_names = mapping\n",
    "    else:\n",
    "        learning_level_names = {'1': '1:Core', '2':'2:Intermediate', '3':'3:Advanced',\n",
    "                    '4':'4:Expert', 'SCM Essentials':'1:Core', '5':'5:Others'}\n",
    "    df[col] = df[col].str.split(' ').str[0]\n",
    "    df[col] = np.where(df[col] == '', 'Missing Info', df[col])\n",
    "    df[col] = df[col].map(learning_level_names)  \n",
    "    df[col] = df[col].fillna(value='Missing Info')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wf\n",
    "datapipe.entity = 'assets'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pydyverse\n",
    "data_definition = None\n",
    "#pydyverse\n",
    "@pf.register_dataframe_method\n",
    "def load_data_definition(entity):\n",
    "    global data_definition\n",
    "    data_definition = azure_blob_storage.blob_to_df(container_name=f\"{datapipe.service}-data-definitions\",\n",
    "                                                        blob_name=f\"{datapipe.service}-data-definition.xlsx\",)\n",
    "    #TODO: to be fixed\n",
    "    data_definition.unique_identifier = data_definition.unique_identifier.astype('bool')\n",
    "    data_definition = data_definition[data_definition.entity == entity]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data_definition(entity=datapipe.entity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vfu\n",
    "@pf.register_dataframe_method\n",
    "def step_load_entity_definition(df, all_entities=False):\n",
    "    \"\"\"loads data definition of entity\"\"\"\n",
    "    datapipe.meta = azure_blob_storage.blob_to_df(container_name=f\"{datapipe.service}-data-definitions\",\n",
    "                                                        blob_name=f\"{datapipe.service}-data-definition.xlsx\",)\n",
    "    if not all_entities:\n",
    "        datapipe.meta = datapipe.meta[datapipe.meta.entity == datapipe.entity]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vfu\n",
    "@pf.register_dataframe_method\n",
    "def step_load_raw_data(df):\n",
    "    df = azure_blob_storage.blobs_to_df(\n",
    "        container_name=f\"{datapipe.service}-{datapipe.entity}-raw\"\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vfu\n",
    "@pf.register_dataframe_method\n",
    "def step_rename_columns(df):\n",
    "    \"\"\"Rename columns (raw column names -> clean column names)\"\"\"\n",
    "    column_names_mapping = dict(\n",
    "        zip(\n",
    "            data_definition[\"column_name_raw\"].values,\n",
    "            data_definition[\"column_name_clean\"].values,\n",
    "        )\n",
    "    )\n",
    "    df = df.rename(columns=column_names_mapping)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wf\n",
    "items_raw = (pd.DataFrame()\n",
    "             .step_load_raw_data()\n",
    "             .step_rename_columns()\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pydyverse\n",
    "@pf.register_dataframe_method\n",
    "def step_replace_missing_values_with_nan(df, missing_values=None):\n",
    "    \"\"\"Replace all missing values with nan\"\"\"\n",
    "    if missing_values is None:\n",
    "        missing_values = [\"nan\", \"NAN\", \"NaN\", \"none\", \"None\", \"NONE\", '\"\"', \"''\", \"\"]\n",
    "    for col in df.select_dtypes(include=\"object\").columns:\n",
    "        df.loc[df[col].isin(missing_values), col] = np.nan\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vfu\n",
    "#TODO: write cleaner version\n",
    "@pf.register_dataframe_method\n",
    "def step_validate_data(df):\n",
    "    \"\"\"\n",
    "    * replace missing with nan\n",
    "    * validate unique identifiers and cardinality\n",
    "    * generated dataframe of invalid entries \n",
    "    * writes data back to storage\n",
    "    \"\"\"\n",
    "    # Remove all empty strings with nan\n",
    "    unique_identifiers = list(\n",
    "        data_definition[data_definition[\"unique_identifier\"] == True][\"column_name_clean\"].values\n",
    "    )\n",
    "    validation_df = pd.DataFrame(columns=df.columns)\n",
    "    validation_df[\"validation_error\"] = None\n",
    "    for unique_identifier in unique_identifiers:\n",
    "        missing_df = df[unique_identifier].isna()\n",
    "        duplicate_df = df[\n",
    "            df[unique_identifier].isin(\n",
    "                df[unique_identifier]\n",
    "                .value_counts()[df[unique_identifier].value_counts() > 2]\n",
    "                .index\n",
    "            )\n",
    "        ]\n",
    "        if missing_df.shape[0] > 0:\n",
    "            missing_df[\"validation_error\"] = f\"Missing {unique_identifier}\"\n",
    "            validation_df = pd.concat([missing_df])\n",
    "        if duplicate_df.shape[0] > 0:\n",
    "            duplicate_df[\"validation_error\"] = f\"Duplicate {unique_identifier}\"\n",
    "            validation_df = pd.concat([duplicate_df])\n",
    "    return validation_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vfu\n",
    "@pf.register_dataframe_method\n",
    "def step_store_invalid_data(df):\n",
    "    df_name = f\"{datapipe.service}-{datapipe.entity}-quality\"\n",
    "    azure_blob_storage.df_to_blob(\n",
    "        container_name=f\"{datapipe.service}-{datapipe.entity}-quality\",\n",
    "        blob_name=f\"{datapipe.service}-{df_name}-quality-{datapipe.today}.csv\",\n",
    "        df=df,\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_definition[data_definition[\"unique_identifier\"] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wf\n",
    "items_invalid = (items_raw\n",
    "                 .step_replace_missing_values_with_nan()\n",
    "                 .step_validate_data()\n",
    "                 .step_store_invalid_data()\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pydyverse\n",
    "@pf.register_dataframe_method\n",
    "def step_remove_not_needed_columns(df):\n",
    "    \"\"\"Remove not needed columns\"\"\"\n",
    "    to_be_removed_columns = list(\n",
    "        data_definition[data_definition.to_be_removed == True][\n",
    "            \"column_name_clean\"\n",
    "        ].values\n",
    "    )\n",
    "\n",
    "    df = df.drop(columns=to_be_removed_columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pydyverse\n",
    "@pf.register_dataframe_method\n",
    "def step_remove_rows_with_invalid_ids(df):\n",
    "    \"\"\"Remove rows with invalid ids\"\"\"\n",
    "    unique_identifiers = list(\n",
    "        data_definition[data_definition.unique_identifier == True][\n",
    "            \"column_name_clean\"\n",
    "        ].values\n",
    "    )\n",
    "    df = df.dropna(subset=unique_identifiers)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pydyverse\n",
    "@pf.register_dataframe_method\n",
    "def step_remove_duplicate_rows(df, keep=\"last\"):\n",
    "    \"\"\"Remove duplicate rows\"\"\"\n",
    "    unique_identifiers = data_definition[data_definition.unique_identifier == True][\"column_name_clean\"].values\n",
    "    df.drop_duplicates(subset=unique_identifiers, keep=keep, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pydyverse\n",
    "@pf.register_dataframe_method\n",
    "def step_format_dates(df, cols, date_format=None):\n",
    "    for col in cols:\n",
    "        if date_format:\n",
    "            df[col] = pd.to_datetime(df[col], format=date_format)\n",
    "        else:\n",
    "            df[col] = pd.to_datetime(df[col], infer_datetime_format=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pydyverse\n",
    "@pf.register_dataframe_method\n",
    "def step_format_round_numeric_cols(df, cols, decimal_places):\n",
    "    for col in cols:\n",
    "        df[col] = df[col].astype(float)\n",
    "        df[col] = df[col].round(decimal_places)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pydyverse\n",
    "@pf.register_dataframe_method\n",
    "def step_change_datatypes(df, date_format=\"%Y-%m-%d %H:%M:%S\"):\n",
    "    \"\"\"Change datatypes according to data definition\"\"\"\n",
    "    _data_definition = data_definition[data_definition.to_be_removed == False]\n",
    "    cols = list(\n",
    "        zip(\n",
    "            _data_definition[\"column_name_clean\"].values,\n",
    "            _data_definition[\"datatype\"].values,\n",
    "        )\n",
    "    )\n",
    "    for col in cols:\n",
    "        if col[1] == \"int\":\n",
    "            df[col[0]] = df[col[0]].astype(float)\n",
    "        elif col[1] == \"str\":\n",
    "            df[col[0]] = df[col[0]].astype(str)\n",
    "        elif col[1] == \"category\":\n",
    "            df[col[0]] = df[col[0]].astype(\"category\")\n",
    "        elif col[1] == \"bool\":\n",
    "            df[col[0]] = df[col[0]].astype(bool)\n",
    "        elif col[1] == \"datetime\":\n",
    "            df[col[0]] = df[col[0]].astype(str)\n",
    "            df[col[0]] = pd.to_datetime(df[col[0]])\n",
    "        elif col[1] == \"date\":\n",
    "            df[col[0]] = df[col[0]].astype(str)\n",
    "            date_format = date_format.split(\" \")[0]\n",
    "            df[col[0]] = pd.to_datetime(df[col[0]])\n",
    "            df[col[0]] = df[col[0]].dt.date\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vfu\n",
    "@pf.register_dataframe_method\n",
    "def step_vfu_save_cleaned_data(df):\n",
    "    azure_blob_storage.df_to_blob(\n",
    "        container_name=f\"{datapipe.service}-{datapipe.entity}-clean\",\n",
    "        blob_name=f\"{datapipe.service}-{datapipe.entity}-clean.csv\",\n",
    "        df=df,\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_cleaned = (items_raw\n",
    "                 .step_replace_missing_values_with_nan()\n",
    "                 .step_remove_not_needed_columns()\n",
    "                 .step_remove_rows_with_invalid_ids()\n",
    "                 .step_remove_duplicate_rows()\n",
    "                 .step_format_dates(cols=['created'])\n",
    "                 .step_format_dates(cols=['modified'])\n",
    "                 .step_format_round_numeric_cols(cols=['average_rating'], decimal_places=2)\n",
    "                 .step_change_datatypes()\n",
    "                 .step_consolidate_item_learning_levels(col='learning_level', mapping=None)\n",
    "                 .step_vfu_save_cleaned_data()\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_cleaned.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
